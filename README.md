# ç¼–å†™äººï¼šwstki
1. å¯¹äºæ¯ä¸ªå‡½æ•°çš„æ„å»ºè¿‡ç¨‹è¿›è¡Œè§£é‡Š
2. æ•°æ®è·å–è¿‡ç¨‹ä¸­æœ‰ä»€ä¹ˆå€¼å¾—è®°å½•çš„åœ°æ–¹
3. æœ‰ä»€ä¹ˆç»éªŒæ•™è®­

# æ¯ä¸ªå‡½æ•°çš„æ„å»ºè¿‡ç¨‹è¿›è¡Œæ€»ç»“
å‚è€ƒç½‘ç«™ï¼š
1.  [æ‰€æœ‰å‡½æ•°çš„åŸå§‹å‚è€ƒï¼Œä½†æ˜¯ä»£ç å¹´ä¹…å¤±ä¿®ï¼Œéœ€è¦å¤§å¹…æ›´æ”¹](https://blog.csdn.net/yuchunyu97/article/details/89504740)
2. [éƒ¨åˆ†å‚è€ƒäº†è¿™äº›è¿™ä¸ªï¼Œä½†æ˜¯ä¹Ÿæ˜¯ç”±äºç«¯å£å¹´ä¹…å¤±ä¿®ï¼Œéœ€è¦æ›´æ”¹](https://blog.csdn.net/u011291072/article/details/81266372)
3. [çˆ¬è™«æœºåˆ¶çš„ç»•è¿‡åŠæ³•ï¼Œååˆ†å¥½ç”¨ï¼Œå»ºè®®æ”¶è—](https://github.com/xiantang/Spider)

æŠ€æœ¯è·¯çº¿ï¼š
1. ruquest+beautifulsoupï¼ˆä¸‹é¢ç§°ä¸ºbsï¼‰**åŸºç¡€çš„çˆ¬è™«ï¼Œä½†æ˜¯æ— æ³•ç»•è¿‡æœåŠ¡å™¨**
2. selenium+bs **æ¨¡æ‹ŸçœŸäººè¿›è¡Œæµè§ˆå™¨æ“ä½œï¼Œè¿›è¡Œæ— å¤´éª‘å£«çš„æ•°æ®è·å–ï¼ˆå¤§é›¾ï¼‰

æŠ€å·§
1. headersçš„è®¾ç½®
## function01 get encrypted_string
### åŸºç¡€çŸ¥è¯† 1 md5 æ˜¯ä»€ä¹ˆ
[å…³äºmd5å’Œsha256çš„ä»‹ç»](https://zhuanlan.zhihu.com/p/510264441)

ç®€å•æ¥è¯´ï¼Œæ¯æ¬¡ç™»é™†åˆ°é©¬èœ‚çªï¼Œç”¨æˆ·éƒ½ä¼šå°†è‡ªå·±çš„è´¦æˆ·ä¿¡æ¯ä»¥md5çš„åŠ å¯†å½¢å¼ä¸Šä¼ åˆ°æœåŠ¡å™¨ç«¯ï¼ŒæœåŠ¡å™¨ä¼šå¯¹æ¯”è‡ªèº«æ•°æ®åº“ä¸­å¯¹åº”ç”¨æˆ·çš„æ¶ˆæ¯å’Œä¸Šä¼ çš„æ¶ˆæ¯æ˜¯å¦ä¸€è‡´ï¼Œè¿™ä¹Ÿæ­£æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ„å»ºå‡½æ•°1 ä»¥é€šè¿‡æœåŠ¡å™¨çš„æ£€éªŒ

### ä»£ç è§£é‡Š

~~~ python
import re  
import certifi  
  
# å°è¯•ç‰ˆæœ¬ç®¡ç†  
def _get_md5_encrypted_string(REQ):  
    """  
    è·å–åŠ å¯†å­—ç¬¦ä¸²  
    :param REQ:    ä½¿ç”¨requestçš„secession
    :return: åŠ å¯†å­—ç¬¦ä¸²  
    """    # ä»¥åŒ—äº¬æ™¯ç‚¹ä¸ºä¾‹ï¼Œé¦–å…ˆè·å–åŠ å¯† js æ–‡ä»¶çš„åœ°å€  
    url = 'https://www.mafengwo.cn/jd/10065/gonglve.html'  
    r = REQ.get(url,verify=certifi.where())  
  
    if r.status_code == 403:  
        exit('è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥æ˜¯å¦ä¸ºIPåœ°å€è¢«ç¦')  
    param = re.findall(  
        r'src="https://js.mafengwo.net/js/hotel/sign/index.js(.*?)"', r.text)  
    param = param[0]  
    # æ‹¼æ¥ index.js çš„æ–‡ä»¶åœ°å€  
    url_indexjs = 'https://js.mafengwo.net/js/hotel/sign/index.js' + param  
    # è·å– index.js    r = REQ.get(url_indexjs)  
    if r.status_code == 403:  
        exit('è®¿é—®è¢«æ‹’ç»')  
    response_text = r.text  
    # æŸ¥æ‰¾åŠ å¯†å­—ç¬¦ä¸²  
    result = re.findall(r'var __Ox2133f=\[(.*?)\];', response_text)[0]  
    byteslike_encrypted_string = result.split(',')[46].replace('"', '')  
    # è§£ç   
    strTobytes = []  
    for item in byteslike_encrypted_string.split('\\x'):  
        if item != '':  
            num = int(item, 16)  
            strTobytes.append(num)  
    # è½¬æ¢å­—èŠ‚ä¸ºå­—ç¬¦ä¸²  
    encrypted_string = bytes(strTobytes).decode('utf8')  
    encrypted_string = encrypted_string  
    return encrypted_string
~~~

1. é¦–å…ˆéœ€è¦å»ºç«‹ä¸€ä¸ªé©¬èœ‚çªè´¦æˆ·ï¼Œå¹¶ä¸”ä¿è¯å…¶å¯ç”¨
2. ä½¿ç”¨requestè¿›è¡Œç™»é™†ï¼Œè·å–index.jsï¼Œè¿™ä¸ªjså…¶ä¸­æœ‰è‡ªå·±çš„ç™»é™†ä¿¡æ¯ï¼Œæ ¹æ®å¯¹äºmd5çš„ä»‹ç»ï¼Œå¯ç”¨ä½¿ç”¨è¿™ä¸ªjsæ¥æ¨¡æ‹Ÿç”¨æˆ·çš„ç™»é™†
3. å°†index.jsä¸­çš„æ•°æ®è¿›è¡Œè§£å¯†å¤„ç†ï¼ˆè¿™é‡Œæ˜¯é«˜ç§‘æŠ€çš„ä¸€éƒ¨åˆ†ï¼Œç†è§£å¾—ä¸å¤ªé€å½»ï¼‰ï¼Œåº”è¯¥ä¸æ˜¯ç›´æ¥çš„è§£å¯†ï¼Œè€Œæ˜¯å°†å­—ç¬¦ä¸²ä¸­çš„ä¸€äº›ç‰¹æ®Šç¬¦å·æ¢æ‰ï¼Œå› ä¸ºæœ€åè¿”å›çš„`encrypted_string`å®é™…ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²

å‡½æ•°1æœ€åè¿”å›çš„æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
## function02 md5 getcityPOIList
ä¸»è¦åªè®²è§£å…¶ä¸­çš„   md5 å‡½æ•°

### headersï¼ˆè¯·æ±‚å¤´ï¼‰çš„è®¾ç½® 
è¿™é‡Œæ˜¯headdersçš„ç¬¬ä¸€æ¬¡è®¾ç½®ï¼Œåé¢è¿˜ä¼šä¸ºheadersçš„è®¾ç½®äº¤å¾ˆå¤šçš„å­¦è´¹
~~~ python
# æœ€ç®€å•çš„headers
HEADERS = headers = {  
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '  
                  'Chrome/75.0.3770.142 Safari/537.36'}
~~~


ç®€å•è®²ä¸€ä¸‹headersçš„ä½œç”¨ï¼Œå®é™…ä¸Šheadersæ˜¯ç”¨æˆ·ç”¨äºè¯·æ±‚æœåŠ¡ç«¯çš„æ•°æ®ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªä½œç”¨ï¼š
1. æœåŠ¡å™¨åˆ¤æ–­ç”¨æˆ·çš„ç»ˆç«¯ç±»å‹ï¼Œæ¯”å¦‚ä¸Šé¢çš„headersï¼Œå°±å°†è‡ªå·±ä¼ªè£…æˆäº†ä¸€ä¸ªwin10çš„chromeæµè§ˆå™¨
2. å¯¹äºæˆ‘ä»¬æ¥è¯´ï¼Œå› ä¸ºå¤§é‡ç½‘é¡µä½¿ç”¨user-agentè¿™ä¸ªå‚æ•°æ¥åˆ¤æ–­æ˜¯ä¸æ˜¯çˆ¬è™«åœ¨è¯·æ±‚æœåŠ¡å™¨ï¼Œä¸€æ¬¡åˆ›å»ºä¸€ä¸ªheadersæ˜¯å¾ˆæœ‰å¿…è¦çš„

### ä»£ç è§£é‡Š
~~~ python
from functions.function_01_get_encrypted_string import _get_md5_encrypted_string  
import time  
import hashlib  
import json  
import requests  
HEADERS = headers = {  
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '  
                  'Chrome/75.0.3770.142 Safari/537.36'}  
REQ = requests.session()  
REQ.headers=HEADERS  
encrypted_string = _get_md5_encrypted_string(REQ)  
def _md5(data):  
    '''  
    è·å–è¯·æ±‚å‚æ•°ä¸­çš„åŠ å¯†å‚æ•°ï¼Œ_ts å’Œ _sn    '''    _ts = int(round(time.time() * 1000))  
    data['_ts'] = _ts  
    # æ•°æ®å¯¹è±¡æ’åºå¹¶å­—ç¬¦ä¸²åŒ–  
    orderd_data = _stringify(data)  
    # md5 åŠ å¯†  
    m = hashlib.md5()  
    m.update((json.dumps(orderd_data, separators=(',', ':')) +  
              encrypted_string).encode('utf8'))  
    _sn = m.hexdigest()  
    # _sn æ˜¯åŠ å¯†åå­—ç¬¦ä¸²çš„ä¸€éƒ¨åˆ†  
    orderd_data['_sn'] = _sn[2:12]  
    return orderd_data  
  
# _md5çš„å‰ç½®å‡½æ•°  
def _stringify(data):  
    """  
    å°† dict çš„æ¯ä¸€é¡¹éƒ½å˜æˆå­—ç¬¦ä¸²  
    """    data = sorted(data.items(), key=lambda d: d[0])  
    new_dict = {}  
    for item in data:  
        if type(item[1]) == dict:  
            # å¦‚æœæ˜¯å­—å…¸ç±»å‹ï¼Œå°±é€’å½’å¤„ç†  
            new_dict[item[0]] = json.dumps(  
                _stringify(item[1]), separators=(',', ':'))  
        else:  
            if type(item[1]) == list:  
                # å¦‚æœæ˜¯åˆ—è¡¨ç±»å‹ï¼Œå°±æŠŠæ¯ä¸€é¡¹éƒ½å˜æˆå­—ç¬¦ä¸²  
                new_list = []  
                for i in item[1]:  
                    new_list.append(_stringify(i))  
                new_dict[item[0]] = new_list  
            else:  
                if item[1] is None:  
                    new_dict[item[0]] = ''  
                else:  
                    new_dict[item[0]] = str(item[1])  
    return new_dict
~~~
å®é™…ä¸Šæˆ‘ä»¬å¹¶ä¸æ˜¯ç›´æ¥å°†function1ä¸­å¾—åˆ°çš„å­—ç¬¦ä¸²ä½œä¸ºæ•°æ®è¾“å…¥æ”¾åˆ°requestsä¸­ç„¶åè¯·æ±‚æ•°æ®ï¼Œè€Œæ˜¯å°† å­—ç¬¦ä¸²encrypted_string + ts
(è¯·æ±‚çš„æ—¶é—´)+snï¼ˆåº”è¯¥æ˜¯ä¸€éƒ¨åˆ†çš„è¯·æ±‚å­—ç¬¦ä¸²ï¼‰ä¸€èµ·ä½œä¸ºä¸€ä¸ªpostdataæ”¾åˆ°requestsä¸­


## function 03 ï¼ˆå·²ç»å¼ƒç”¨ï¼‰
å‡½æ•°3åŸæœ¬æ˜¯é€šè¿‡ http://pagelet.mafengwo.cn/poi/pagelet/poiLocationApi æ¥è·å–æ¯ä¸ªpoiçš„ç»çº¬åº¦æ•°æ®çš„ï¼Œä½†æ˜¯æœåŠ¡å™¨ç«¯å·²ç»æ‹’ç»äº†è®¿é—®ï¼Œæˆ‘ä¹Ÿæ²¡æœ‰åŠæ³•ç»•è¿‡ï¼Œæ‰€ä»¥è¿™ä¸ªå‡½æ•°åªèƒ½ä½œåºŸï¼Œä½†æ˜¯é€šè¿‡å‡½æ•°çš„è¯¦ç»†åœ°å€ï¼Œèƒ½å¤Ÿè¿›è¡Œåœ°ç†ç¼–ç ï¼Œæ ¹æ®POIçš„é—¨ç‰Œå·è·å–å¯¹åº”çš„ç»çº¬åº¦ï¼Œå¹¶ä¸”è¿›è¡Œåæ ‡è½¬æ¢ï¼ˆæ¥è‡ªåœ°ä¿¡å­¦ç”Ÿçš„è‡ªä¿¡ï¼‰

## function 04 transCoordinate
[å‚è€ƒç½‘å€](https://developer.aliyun.com/article/1281221)

çœ‹åå­—å°±çŸ¥é“ï¼Œè¿™æ˜¯ä¸€ä¸ªåæ ‡è½¬æ¢çš„å‡½æ•°ï¼Œfunction4çš„æ•´ä¸ªpyå®ç°äº†ä¸¤ä¸ªåŠŸèƒ½ï¼š
1. åœ°ç†ç¼–ç ï¼Œå°†åœ°å€ç¼–ç ä¸ºbd09æˆ–è€…gcj02çš„ç»çº¬åº¦
2. ç»çº¬åº¦è½¬æ¢ï¼Œå°†bd09æˆ–è€…gcj02çš„åæ ‡è½¬ä¸ºwgs84çš„åæ ‡

psï¼š è°ƒç”¨è…¾è®¯åœ°å›¾æˆ–è€…ç™¾åº¦åœ°å›¾çš„api keyéœ€è¦è‡ªè¡Œè·å–

~~~ python
def tx_geoCoordinate(addr):  
    with open(r'E:\pycharm\keys\tx_key.txt','r',encoding='utf-8') as file:  
        key = file.read()  
    #æŸ¥è¯¢addrçš„ç»çº¬åº¦  
    template = f'https://apis.map.qq.com/jsapi?qt=geoc&addr={addr}&key={key}=jsonp&pf=jsapi&ref=jsapi&cb=qq.maps._svcb2.geocoder0'  
    url = template.format(addr=addr)  
    resp = requests.get(url)  
    lon = float(re.findall('pointx":"(.*?)",', resp.text)[0])  
    lat = float(re.findall('pointy":"(.*?)",', resp.text)[0])  
    return lon,lat  
  
  
# å¤‡ç”¨ï¼Œç”¨ç™¾åº¦åœ°å›¾è·å–åœ°ç†ç¼–ç   
def bd_geoCoordinate(addr):  
    with open(r'E:\pycharm\keys\bd_key.txt','r',encoding='utf-8') as file:  
        key = file.read()  
    ua = UserAgent()  
  
    ua_get = ua.random  
  
    header_get = {  
        'User-Agent': ua_get,  
        'Connection': "close"  
    }  
    # ç™¾åº¦æä¾›çš„æ¥å£  
    url = 'http://api.map.baidu.com/geocoding/v3/?address='  
    # æ•°æ®  
    output = 'json'  
    # KEYè¦å»ç™¾åº¦åœ°å›¾å¼€å‘è€…å¹³å°ç”³è¯·  
    ak = key  
    # é€šè¿‡ç™¾åº¦åˆ›å»ºåº”ç”¨å¾—åˆ°akï¼Œè®°å¾—å†™ä¸ºæµè§ˆå™¨ç«¯ï¼Œç„¶åå†™*  
    addr = quote(str(addr))  
    uri = url + addr + '&output=' + output + "&ak=" + ak  
    try:  
        req = requests.get(uri, headers=header_get).text  
        #         print( req)  
  
        # res = req.read().decode()        time.sleep(0.1)  
        # print(res)  
        temp = json.loads(req)  
        if temp['status'] == 0:  
            # ç²¾åº¦  
            lat = temp['result']['location']['lat']  
            # çº¬åº¦  
            lng = temp['result']['location']['lng']  
  
            #  ç™¾åº¦åœ°å›¾è§£æç»“æœç²¾åº¦åˆ¤æ–­ä»¥å­—æ®µcomprehensionçš„å€¼ä¸ºä¾æ®  
            acc = temp['result']['comprehension']  
            use = temp['result']['level']  
        else:  
            lat = 0  
            lng = 0  
            acc = "æ— "  
            use = 'æ— '  
        # return lat, lng,acc  
        return lng, lat, acc, use  
  
    except Exception as e:  
        print(e)  
        #         print(data['åœ°å€1'].iloc[i])  
        time.sleep(3)  
        bd_geoCoordinate(addr)

~~~
æä¾›äº†ä¸¤ä¸ªè¿›è¡Œåœ°ç†ç¼–ç çš„æ–¹å¼ï¼Œå› ä¸ºåœ°ç†ç¼–ç çš„apiæ˜¯æœ‰é™åˆ¶çš„ï¼Œå¦‚æœèƒ½å¤Ÿäº¤æ›¿ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„åœ°å›¾è¿›è¡Œåœ°ç†ç¼–ç å°±èƒ½æ•ˆç‡æœ€å¤§åŒ–ï¼ˆéœ²å‡ºäº†è´«ç©·çš„ç¬‘å®¹ğŸ˜€ï¼‰ï¼Œæˆ–è€…æ‰¾å‡ ä¸ªèº«è¾¹äººå€Ÿä¸€ä¸‹keyä¹Ÿæ˜¯å¯ä»¥çš„

ä¸Šé¢çš„ä¸¤ä¸ªå‡½æ•°éƒ½ååˆ†ç®€å•ï¼Œå°±æ˜¯è®¿é—®åœ°å›¾apiï¼Œä¸€èˆ¬è¿”å›çš„æ˜¯jsonæ•°æ®ï¼Œä½¿ç”¨jsonè¿›è¡Œè§£è¯»ï¼Œå°±èƒ½å¾—åˆ°bd09æˆ–è€…gcj02çš„åæ ‡ï¼Œåé¢ç”¨ç±»From_gcj02_to_wgs84ï¼ˆæˆ–è€…æ˜¯From_bd09_to_wgs84ï¼‰ä¸­çš„gcj02_to_wgs84(æˆ–bd09_to_wgs84)å‡½æ•°å°±èƒ½è¿›è¡Œè½¬æ¢

## function04 getCitiesData
è™½ç„¶è¿™ä¸ªå‡½æ•°ä¹Ÿå«04ï¼Œä½†æ˜¯å’Œä¸Šé¢çš„åœ°ç†ç¼–ç æ¯«æ— å¹²ç³»ï¼Œæ›´åƒæ˜¯åé¢çš„function05

å‡½æ•°ä½œç”¨ï¼šè·å–å…¨å›½çƒ­é—¨æ—…æ¸¸åŸå¸‚çš„å¯¹åº”id
### ä¸¾å‡ºä¸€ä¸ªä¾‹å­
è¿™ä¸ªå‡½æ•°æ˜¯é’ˆå¯¹é©¬èœ‚çªçš„[ç›®çš„åœ°ä¸»é¡µ](https://www.mafengwo.cn/mdd/)æ¥è®¾è®¡çš„ï¼Œé™¤äº†è·å–å›½å†…åŸå¸‚çš„idï¼Œé€šè¿‡ä¿®æ”¹å‡½æ•°ï¼Œè¿˜èƒ½å¤Ÿè·å–å…¶ä»–å›½å¤–åŒºåŸŸçš„æ•°æ®

![[Pasted image 20240209190539.png]]
é€šè¿‡f12å¯ä»¥å¿«é€Ÿæ‰¾åˆ°è¦è·å–çš„htmlä¸­çš„ä¸€ä¸ªåˆ—è¡¨ï¼ˆè¿™ä¸ªåˆ—è¡¨ä»¥åŠè—èµ·æ¥çš„åˆ—è¡¨éƒ½æ˜¯å¯ä»¥ç›´æ¥è·å–çš„ï¼‰

### å‡½æ•°è§£æ
~~~ python
# name:function_04_getCitiesData  
# æµ‹è¯•ï¼ŒæŸ¥çœ‹ä¸€ä¸ªåŸå¸‚ä¸­çš„POIçš„æ•°é‡  
from functions.function_06_getHTML import html_crawler  
from bs4 import BeautifulSoup  
import pandas as pd  
import os  
  
#  æå–åŸå¸‚æ•°æ®  
# æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨  
# todo åé¢å¦‚æœå»ºç«‹äº†æ•°æ®åº“ï¼Œå°±ä¸ç”¨æ£€æŸ¥csvæ–‡ä»¶äº†ï¼Œcacheæ–‡ä»¶å¤¹ä¹Ÿä¸éœ€è¦å­˜åœ¨äº†ï¼Œåé¢ä¹Ÿä¼šå°†è¿™ä¸ªå‡½æ•°çš„è¾“å‡ºç›´æ¥è¿æ¥åˆ°æ•°æ®åº“  
  
def getCitiesData():  
    csv_file_path = os.path.join('..', 'cache', 'cities_data.csv')  
  
    if os.path.exists(csv_file_path):  
        print("åŸå¸‚æ•°æ®å·²å­˜åœ¨ï¼Œæ— éœ€å†æ¬¡æå–ã€‚")  
  
        exit()  
    else:  
        print("æ­£åœ¨æå–åŸå¸‚æ•°æ®...")  
        url = 'https://www.mafengwo.cn/mdd/'  
        html_content = html_crawler(url)  
        soup = BeautifulSoup(html_content, 'html.parser')  
        # æ‰¾åˆ°åŒ…å«åŸå¸‚ä¿¡æ¯çš„<div>æ ‡ç­¾  
        hot_list_div = soup.find('div', class_='hot-list clearfix')  
        # æ‰¾åˆ°<div>ä¸‹çš„æ‰€æœ‰<dd>æ ‡ç­¾  
        dd_tags = hot_list_div.select('dd')  
        # æå–åŸå¸‚ä¿¡æ¯  
        # æå–åŸå¸‚ä¿¡æ¯å¹¶è¾“å‡ºä¸ºå­—å…¸åˆ—è¡¨  
        cities_list = []  
  
        for dd_tag in dd_tags:  
            # æ‰¾åˆ°<dd>æ ‡ç­¾ä¸­çš„æ‰€æœ‰åŸå¸‚é“¾æ¥  
            city_links = dd_tag.find_all('a', target='_blank')  
  
            # æ„å»ºå­—å…¸åˆ—è¡¨  
            city_dicts = [{'city': link.get_text(strip=True),  
                           'link': link['href'],  
                           'poi_count': 0,  
                           'id': link['href'].rsplit('/', 1)[-1].rsplit('.', 1)[0]}  
                          for link in city_links  
                          ]  
  
            # å°†å½“å‰<dd>æ ‡ç­¾ä¸­çš„åŸå¸‚å­—å…¸åˆ—è¡¨åŠ å…¥æ€»åˆ—è¡¨  
            cities_list.extend(city_dicts)  
  
        # å°†å­—å…¸åˆ—è¡¨è½¬ä¸ºDataFrame  
        df = pd.DataFrame(cities_list)  
        # å°†DataFrameå¯¼å‡ºä¸ºCSVæ–‡ä»¶  
        df.to_csv('cities_data.csv', index=False)  
        print("åŸå¸‚æ•°æ®æå–å®Œæˆã€‚")  
        # å‘dfä¸­æ·»åŠ poiæ•°é‡ï¼Œç”±äºæ¯æ¬¡éƒ½è¦è¯»ä¸€ä¸ªç½‘é¡µï¼Œæ‰€ä»¥éœ€è¦å°†csvæ¯æ¬¡éƒ½è¿›è¡Œä¿å­˜  
  
        cities_df = pd.read_csv(csv_file_path)  
        ids = cities_df['id']  
        # å»æ‰ä¸ºç©ºçš„é¡¹  
        ids = ids.dropna()  
        for city_id in ids:  
            if city_id is not None:  
                city_name = cities_df.loc[cities_df['id'] == city_id, 'city'].iloc[0]  
            else:  
                continue  
            try:  
                city_id_str = str(city_id).split('.')[0]  
                # æ„å»ºå‡½æ•°ï¼Œè¾“å…¥åŸå¸‚åï¼ŒæŸ¥æ‰¾è¿™ä¸ªåŸå¸‚çš„æ™¯ç‚¹æ•°é‡  
                url = f'https://www.mafengwo.cn/jd/{city_id_str}/gonglve.html'  
                # ä½¿ç”¨æ¡ä»¶ç­›é€‰è·å–åŸå¸‚åç§°  
  
                # æ£€æŸ¥ 'poi_count' å¯¹åº”çš„å€¼æ˜¯å¦ä¸º0  
                index_to_check = cities_df.index[cities_df['id'] == city_id].tolist()[0]  
                if cities_df.loc[index_to_check, 'poi_count'] == 0:  
                    html_content = html_crawler(url)  
                    soup = BeautifulSoup(html_content, 'html.parser')  
                    # æ‰¾åˆ°åŒ…å«é¡µæ•°å’Œæ¡ç›®æ•°çš„<span>å…ƒç´   
                    count_span = soup.find('span', class_='count')  
                    # æ‰¾åˆ°åµŒå¥—çš„æ‰€æœ‰<span>å…ƒç´   
                    nested_spans = count_span.find_all('span')  
                    # æå–æ€»æ¡ç›®æ•°ï¼Œä½äºç¬¬äºŒä¸ªåµŒå¥—çš„<span>å…ƒç´ ï¼Œå³POIæ•°é‡  
                    item_count = int(nested_spans[1].get_text(strip=True))  
                    # åœ¨å¯¹åº” id åˆ—çš„é‚£ä¸€è¡Œæ·»åŠ å¯¹åº”çš„ poi æ•°é‡  
                    index_to_update = cities_df.index[cities_df['id'] == city_id].tolist()[0]  
                    cities_df.loc[index_to_update, 'poi_count'] = item_count  
                    # ä¿å­˜æ›´æ–°åçš„ DataFrame åˆ° CSV æ–‡ä»¶  
                    cities_df.to_csv(csv_file_path, index=False)  
                    print(f"{city_name} çš„æ™¯ç‚¹æ•°è·å–å®Œæ¯•ã€‚")  
                elif cities_df.loc[index_to_check, 'poi_count'] == -1:  
                    print(f"{city_name} çš„æ™¯ç‚¹æ•°è·å–å¤±è´¥ï¼Œå°†è¯¥åŸå¸‚çš„æ™¯ç‚¹æ•°è®¾ä¸º -1")  
                else:  
                    print(f"{city_name} çš„æ™¯ç‚¹æ•°å·²å­˜åœ¨ï¼Œæ— éœ€å†æ¬¡è·å–ã€‚")  
            except Exception as e:  
                print(f"An error occurred for city_id {city_name}: {str(e)},å°†è¯¥åŸå¸‚çš„æ™¯ç‚¹æ•°è®¾ä¸º -1")  
                # åœ¨å¯¹åº”idåˆ—çš„é‚£ä¸€è¡Œæ·»åŠ  poi_count è®¾ä¸º -1                index_to_update = cities_df.index[cities_df['id'] == city_id].tolist()[0]  
                cities_df.loc[index_to_update, 'poi_count'] = -1  
                # ä¿å­˜æ›´æ–°åçš„ DataFrame åˆ° CSV æ–‡ä»¶  
                cities_df.to_csv(csv_file_path, index=False)
~~~
æ•´ä¸ªå‡½æ•°çœ‹èµ·æ¥å¾ˆé•¿ï¼Œä½†æ˜¯å®é™…ä¸Šååˆ†ç®€å•ï¼Œä¸»è¦å°±æ˜¯ä½¿ç”¨beautifulsoupæ¥è§£æhttps://www.mafengwo.cn/mdd/è¿™ä¸ªç½‘é¡µï¼Œå°†è‡ªå·±æƒ³è¦çš„ä¿¡æ¯è·å–ä¸‹æ¥ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†htmlä¸­classä¸º`hot-list clearfix`çš„divå…ƒç´ æ•´ä½“è·å–ä¸‹æ¥
~~~ python
hot_list_div = soup.find('div', class_='hot-list clearfix')
~~~
å¾—åˆ°çš„å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„åˆ—è¡¨


## function 05 getPOIID
è·å–ä¸€ä¸ªåŸå¸‚ä¸­çš„300ä¸ªåˆå§‹çš„POI

ç”±äºé©¬èœ‚çªç½‘é¡µç‰ˆçš„é™åˆ¶ï¼Œåœ¨ç½‘é¡µç«¯ï¼Œæ— è®ºä¸€ä¸ªåŸå¸‚æœ‰å¤šå°‘ä¸ªPOIï¼Œæœ€åèƒ½å¤Ÿçˆ¬ä¸‹æ¥çš„POIåªæœ‰300ä¸ªæ™¯ç‚¹çš„ID

### ä¸¾å‡ºä¸€ä¸ªä¾‹å­
ä»¥[åŒ—äº¬å¸‚](https://www.mafengwo.cn/jd/10065/gonglve.html)ä¸ºä¾‹
![[Pasted image 20240209184607.png]]
æ¯é¡µæœ‰15ä¸ªæ™¯ç‚¹ï¼Œå…±20é¡µï¼Œä¹Ÿå°±æ˜¯ä¸€å…±300ä¸ªï¼Œä½†æ˜¯æˆªå›¾ä¸­æ˜¾ç¤ºåŒ—äº¬å¸‚æ€»å…±æœ‰15251ä¸ªæ™¯ç‚¹ï¼Œè¿™äº›æ™¯ç‚¹ç†è®ºä¸Šæ˜¯èƒ½å¤Ÿè·å–çš„ï¼Œä½†æ˜¯ä»…å¯¹äºç›®å‰è€Œè¨€æš‚æ—¶æ˜¯æ²¡æœ‰å¿…è¦çš„ï¼Œåé¢ä¹Ÿä¼šç®€å•èŠåˆ°æå–æ‰€æœ‰è¿™äº›æ™¯ç‚¹çš„idçš„å¯èƒ½æ–¹æ³•

~~~ python
import re  
from bs4 import BeautifulSoup  
import requests  
import pandas as pd  
from functions.function_02_md5_getCityPOIList import _md5  
HEADERS = headers = {  
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '  
                  'Chrome/75.0.3770.142 Safari/537.36'}  
REQ = requests.session()  
REQ.headers=HEADERS  
# è·å–ä¸€ä¸ªåŸå¸‚çš„POIå’Œå¯¹åº”çš„ç¼–å·  
def _get_route(mdd_id):  
    '''  
    è·å–æ™¯ç‚¹ä¿¡æ¯  
    '''  
  
    results = []  
    # è·å–æ™¯ç‚¹æœ‰å¤šå°‘é¡µï¼Œé˜²æ­¢å°‘äº20é¡µ  
    post_data = _md5({  
        'sAct': 'KMdd_StructWebAjax|GetPoisByTag',  
        'iMddid': mdd_id,  
        'iTagId': 0,  
        'iPage': 1  
    })  
    url = 'http://www.mafengwo.cn/ajax/router.php' + '?' + '&'.join(  
        [f'{key}={value}' for key, value in post_data.items()])  
    r = REQ.post(url, data=post_data)  
    if r.status_code == 403:  
        exit('è®¿é—®è¢«æ‹’ç»')  
    response = r.json()  
    list_data = response['data']['list']  
    page_data = response['data']['page']  
    soup_page = BeautifulSoup(page_data, "html.parser")  
    page = int(soup_page.find('span', class_='count').find('span').text)  
    # æ²¡æ³•çªç ´20é¡µçš„é™åˆ¶ï¼Œæ¯ä¸ªåŸå¸‚æœ€å¤šåªèƒ½è·å–300ä¸ªPOI  
    for page in range(1,page+1):  
        post_data = _md5({  
            'sAct': 'KMdd_StructWebAjax|GetPoisByTag',  
            'iMddid': mdd_id,  
            'iTagId': 0,  
            'iPage': page  
        })  
        url = 'http://www.mafengwo.cn/ajax/router.php' + '?' + '&'.join([f'{key}={value}' for key, value in post_data.items()])  
        r = REQ.post(url, data=post_data)  
        if r.status_code == 403:  
            exit('è®¿é—®è¢«æ‹’ç»')  
        response = r.json()  
        list_data = response['data']['list']  
        print(list_data)  
        page_data = response['data']['page']  
        # è§£ææ™¯ç‚¹åˆ—è¡¨æ•°æ®  
        soup = BeautifulSoup(list_data, "html.parser")  
        route_list = soup.find_all('a')  
  
        for route in route_list:  
            link = route['href']  
            route_id = re.findall(r'/poi/(.*?).html', link)  
            name = route['title']  
            image = route.find('img')['src'].split('?')[0]  
            results.append({  
                'poi_id': int(route_id[0]),  
                'name': name,  
                'image': image,  
                'link': 'http://www.mafengwo.cn' + link,  
            })  
  
        df = pd.DataFrame(results)  
  
        # è¿”å›å½“å‰é¡µåˆ—è¡¨æ•°æ®å’Œæ€»é¡µæ•°  
        return results,df

~~~






### PS å…³äºPOIIDçˆ¬å–çš„ä¸€äº›æƒ³æ³•
æš‚æ—¶è·³è¿‡


### ä»£ç è§£é‡Š
è·³è¿‡å¯¹From_bd09_to_wgs84å’ŒFrom_gcj02_to_wgs84çš„è§£é‡Š



## è¯´åœ¨æœ€å
1. ä»é›¶æ„å»ºä¸€ä¸ªçˆ¬è™«ç¡®å®æ˜¯ä¸€ä»¶ååˆ†å›°éš¾çš„äº‹æƒ…ï¼Œç«™åœ¨å‰äººçš„è‚©è†€ä¸Šæ‰èƒ½æ›´å¥½çš„è¿›åŒ–å˜¿å˜¿å˜¿
